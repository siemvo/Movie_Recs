{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b09f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vonks\\Documents\\GitHub\\Movie_Recs\\Downloads\n"
     ]
    }
   ],
   "source": [
    "# Data from https://grouplens.org/datasets/movielens/latest/\n",
    "# Put the downloaded csv files in the \"Downloads\" folder\n",
    "\n",
    "import os\n",
    "os.chdir(\"Downloads\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366cf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "data = ratings.merge(movies, on=\"movieId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a84fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  num_ratings\n",
      "0  189614        33332\n"
     ]
    }
   ],
   "source": [
    "# Removing users with more than 10,000 ratings as these are likely to be bots or outliers\n",
    "# See Data_Exploration.ipynb for more details about cleaning and outlier detection\n",
    "\n",
    "ratings_per_user = data.groupby('userId').size()\n",
    "outliers = ratings_per_user[ratings_per_user > 10000]\n",
    "filtered_data = data[~data['userId'].isin(outliers.index)]\n",
    "\n",
    "# Table of removed users\n",
    "removed_users_table = outliers.reset_index()\n",
    "removed_users_table.columns = ['userId', 'num_ratings']\n",
    "\n",
    "print(removed_users_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340b0b4",
   "metadata": {},
   "source": [
    "## User Similarity Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cb32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Parameters\n",
    "min_ratings_per_user = 20 # For selection of active users\n",
    "sample_size = 100\n",
    "K = 10 # Number of recommendations\n",
    "liked_threshold = 4\n",
    "top_n_input = 4 # Number of liked movies to use for similarity\n",
    "\n",
    "# Select active users\n",
    "active_users = filtered_data.groupby('userId').size()\n",
    "active_users = active_users[active_users >= min_ratings_per_user].index\n",
    "\n",
    "# Random sample of users because data is large\n",
    "np.random.seed(123)\n",
    "sampled_users = np.random.choice(active_users, size=sample_size, replace=False)\n",
    "\n",
    "# For each sampled user, split their movie ratings into train (80%) and test (20%)\n",
    "train_ratings = pd.DataFrame()\n",
    "test_ratings = pd.DataFrame()\n",
    "\n",
    "for uid in sampled_users:\n",
    "    user_data = filtered_data[filtered_data['userId'] == uid]\n",
    "    train, test = train_test_split(user_data, test_size=0.2, random_state=42)\n",
    "    train_ratings = pd.concat([train_ratings, train])\n",
    "    test_ratings = pd.concat([test_ratings, test])\n",
    "\n",
    "# -----------------------------\n",
    "# Build matrix from training data\n",
    "# -----------------------------\n",
    "user_ids = train_ratings['userId'].unique()\n",
    "movie_ids = train_ratings['movieId'].unique()\n",
    "user_to_idx = {uid:i for i, uid in enumerate(user_ids)} # Rows\n",
    "movie_to_idx = {mid:i for i, mid in enumerate(movie_ids)} # Columns\n",
    "\n",
    "# Empty matrix\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "user_item_matrix = np.zeros((n_users, n_movies))\n",
    "\n",
    "# Fill matrix\n",
    "for row in train_ratings.itertuples():\n",
    "    u_idx = user_to_idx[row.userId]\n",
    "    m_idx = movie_to_idx[row.movieId]\n",
    "    user_item_matrix[u_idx, m_idx] = row.rating\n",
    "\n",
    "# -----------------------------\n",
    "# Generate recommendations using only top liked movies\n",
    "# -----------------------------\n",
    "user_metrics = []\n",
    "\n",
    "for uid in sampled_users:\n",
    "    u_idx = user_to_idx[uid]\n",
    "    user_vector = user_item_matrix[u_idx, :] # Ratings\n",
    "\n",
    "    # Select top-N liked movies only (positive ratings)\n",
    "    liked_indices = np.where(user_vector >= liked_threshold)[0]\n",
    "    if len(liked_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    top_indices = np.argsort(user_vector[liked_indices])[-top_n_input:]\n",
    "    top_indices = liked_indices[top_indices]\n",
    "\n",
    "    # Find relevant users who rated at least one of these top-N movies\n",
    "    rel_users = np.any(user_item_matrix[:, top_indices] > 0, axis=1)\n",
    "    rel_users[u_idx] = False\n",
    "    relevant_users_matrix = user_item_matrix[rel_users, :]\n",
    "\n",
    "    if relevant_users_matrix.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # Cosine similarity on top-N liked movies\n",
    "    sims = cosine_similarity([user_vector[top_indices]], relevant_users_matrix[:, top_indices])[0]\n",
    "\n",
    "    # Compute weighted average for unseen movies based on similar users = prediction\n",
    "    unseen = np.where(user_vector == 0)[0]\n",
    "    weighted_scores = np.zeros(len(unseen))\n",
    "    for i, m_idx in enumerate(unseen):\n",
    "        weighted_scores[i] = np.sum(relevant_users_matrix[:, m_idx] * sims) / (np.sum(sims) + 1e-8)\n",
    "\n",
    "    # Top recommendations\n",
    "    top_recs_idx = np.argsort(weighted_scores)[::-1][:K] # Top K\n",
    "    recommended_movie_ids = [list(movie_to_idx.keys())[unseen[i]] for i in top_recs_idx]\n",
    "\n",
    "    # Test set \n",
    "    test_all = test_ratings[test_ratings['userId'] == uid]\n",
    "    test_liked = set(test_all[test_all['rating'] >= liked_threshold]['movieId'])\n",
    "    test_not_liked = set(test_all[test_all['rating'] < liked_threshold]['movieId'])\n",
    "\n",
    "    user_metrics.append({\n",
    "        'recommended': recommended_movie_ids,\n",
    "        'test_liked': test_liked,\n",
    "        'test_not_liked': test_not_liked\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations based on user similarity of liked movies:\n",
      "             Recommended  Not Recommended         Total\n",
      "Liked           1.130435        13.684783     14.815217\n",
      "Not Liked       0.543478        15.358696     15.902174\n",
      "Not Watched     8.326087     81781.956522  81790.282609\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies recommended in 10 recs: 0.094\n",
      "Precision in 10 recs: 0.113\n",
      "Recall in 10 recs: 0.094\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "all_movie_ids = set(filtered_data['movieId'].unique())\n",
    "K = 10  # top-K recommendations\n",
    "liked_threshold = 4.0\n",
    "\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "#filtered_user_metrics = [um for um in user_metrics if len(um['test_liked']) > 0]\n",
    "\n",
    "for um in user_metrics:\n",
    "    # Unpack the information per user (um)\n",
    "    recommended = set(um['recommended'])\n",
    "    test_liked = um['test_liked']\n",
    "    test_not_liked = um['test_not_liked']\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    # Metrics for table\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = all_movie_ids - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    # Hit Rate = 1 if at least one liked movie is recommended\n",
    "    #hit = 1 if len(recommended & test_liked) > 0 else 0\n",
    "    hit = len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0\n",
    "\n",
    "    # Precision and Recall K\n",
    "    precision = len(recommended & test_liked) / len(recommended) if len(recommended) > 0 else 0\n",
    "    recall = len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0\n",
    "    \n",
    "    hit_rates.append(hit)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Table with averages\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "average_counts = counts_df.mean()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        average_counts['Recommended_Watched+Liked'],\n",
    "        average_counts['Recommended_Watched+NotLiked'],\n",
    "        average_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        average_counts['NotRecommended_Watched+Liked'],\n",
    "        average_counts['NotRecommended_Watched+NotLiked'],\n",
    "        average_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "\n",
    "# Add totals column\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "print(\"Recommendations based on user similarity of liked movies:\")\n",
    "print(table)\n",
    "\n",
    "avg_hit_rate = np.mean(hit_rates)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies recommended in {K} recs: {avg_hit_rate:.3f}\")\n",
    "print(f\"Precision in {K} recs: {avg_precision:.3f}\")\n",
    "print(f\"Recall in {K} recs: {avg_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066577d",
   "metadata": {},
   "source": [
    "## Recommending based on top 10 rated movies overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139889ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Movies in the Entire Dataset (min 10000 ratings):\n",
      "   movieId                                              title  average_rating  \\\n",
      "0      318                   Shawshank Redemption, The (1994)        4.416791   \n",
      "1   202439                                    Parasite (2019)        4.329973   \n",
      "2      858                              Godfather, The (1972)        4.326600   \n",
      "3     1221                     Godfather: Part II, The (1974)        4.269505   \n",
      "4       50                         Usual Suspects, The (1995)        4.267869   \n",
      "5     1203                                12 Angry Men (1957)        4.267126   \n",
      "6     2019        Seven Samurai (Shichinin no samurai) (1954)        4.250774   \n",
      "7      527                            Schindler's List (1993)        4.242334   \n",
      "8     2959                                  Fight Club (1999)        4.236028   \n",
      "9     5618  Spirited Away (Sen to Chihiro no kamikakushi) ...        4.226042   \n",
      "\n",
      "   num_ratings  \n",
      "0       122295  \n",
      "1        12398  \n",
      "2        75003  \n",
      "3        47270  \n",
      "4        72892  \n",
      "5        22729  \n",
      "6        17119  \n",
      "7        84231  \n",
      "8        86206  \n",
      "9        35374  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "K = 10\n",
    "min_ratings_per_movie = 10000  # threshold applied on the ENTIRE dataset\n",
    "\n",
    "movie_stats_all = filtered_data.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "eligible_movies = movie_stats_all[movie_stats_all['count'] >= min_ratings_per_movie]\n",
    "\n",
    "# Sort by average rating descending\n",
    "top_movies_by_avg_all = eligible_movies.sort_values('mean', ascending=False).head(K)\n",
    "\n",
    "# Merge with movie titles\n",
    "top_movies_summary = top_movies_by_avg_all.merge(movies[['movieId', 'title']], on='movieId', how='left')\n",
    "\n",
    "# Rename columns\n",
    "top_movies_summary.rename(columns={'mean': 'average_rating', 'count': 'num_ratings'}, inplace=True)\n",
    "top_movies_summary = top_movies_summary[['movieId', 'title', 'average_rating', 'num_ratings']]\n",
    "\n",
    "# Display top movies\n",
    "print(f\"Top-{K} Movies in the Entire Dataset (min {min_ratings_per_movie} ratings):\")\n",
    "print(top_movies_summary)\n",
    "\n",
    "# Save movie IDs for later recommendation block\n",
    "top_movie_ids = top_movies_summary['movieId'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1548f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Recommendations Validation for 1000 Users:\n",
      "                     Recommended  Not Recommended      Total\n",
      "Watched + Liked            0.292            9.714     10.006\n",
      "Watched + Not Liked        0.060            8.859      8.919\n",
      "Not Watched                9.648        81792.427  81802.075\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies recommended in 10 recs: 0.050\n",
      "Precision in 10 recs: 0.029\n",
      "Recall in 10 recs: 0.050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# Parameters\n",
    "# --------------------------\n",
    "sample_user_count = 1000          # Change this number to test more users\n",
    "liked_threshold = 4.0\n",
    "test_fraction = 0.2              # 20% test split per user\n",
    "K = len(top_movie_ids)           # ensure consistency with block 1\n",
    "np.random.seed(123)\n",
    "\n",
    "all_movie_ids = set(filtered_data['movieId'].unique())\n",
    "\n",
    "sampled_users = np.random.choice(filtered_data['userId'].unique(),\n",
    "                                 size=min(sample_user_count, filtered_data['userId'].nunique()),\n",
    "                                 replace=False)\n",
    "\n",
    "# --------------------------\n",
    "# Split train/test per sampled user\n",
    "# --------------------------\n",
    "train_ratings = []\n",
    "test_ratings = []\n",
    "\n",
    "for uid in sampled_users:\n",
    "    group = filtered_data[filtered_data['userId'] == uid]\n",
    "    n_test = max(1, int(len(group) * test_fraction))\n",
    "    test_indices = np.random.choice(group.index, size=n_test, replace=False)\n",
    "    train_indices = group.index.difference(test_indices)\n",
    "    \n",
    "    train_ratings.append(filtered_data.loc[train_indices])\n",
    "    test_ratings.append(filtered_data.loc[test_indices])\n",
    "\n",
    "train_ratings = pd.concat(train_ratings)\n",
    "test_ratings = pd.concat(test_ratings)\n",
    "\n",
    "user_metrics_top = []\n",
    "\n",
    "for uid, group in test_ratings.groupby('userId'):\n",
    "    recommended = set(top_movie_ids)\n",
    "    test_liked = set(group[group['rating'] >= liked_threshold]['movieId'])\n",
    "    test_not_liked = set(group[group['rating'] < liked_threshold]['movieId'])\n",
    "    \n",
    "    user_metrics_top.append({\n",
    "        'recommended': recommended,\n",
    "        'test_liked': test_liked,\n",
    "        'test_not_liked': test_not_liked\n",
    "    })\n",
    "\n",
    "# --------------------------\n",
    "# Compute counts and evaluation metrics\n",
    "# --------------------------\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "for um in user_metrics_top:\n",
    "    recommended = set(um['recommended'])\n",
    "    test_liked = um['test_liked']\n",
    "    test_not_liked = um['test_not_liked']\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = all_movie_ids - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    # Hit rate, precision, recall\n",
    "    hit_rates.append(len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "    precisions.append(rec_watched_liked / K)\n",
    "    recalls.append(rec_watched_liked / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "\n",
    "# --------------------------\n",
    "# Table\n",
    "# --------------------------\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "avg_counts = counts_df.mean()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        avg_counts['Recommended_Watched+Liked'],\n",
    "        avg_counts['Recommended_Watched+NotLiked'],\n",
    "        avg_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        avg_counts['NotRecommended_Watched+Liked'],\n",
    "        avg_counts['NotRecommended_Watched+NotLiked'],\n",
    "        avg_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Watched + Liked', 'Watched + Not Liked', 'Not Watched'])\n",
    "\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "avg_hit_rate = np.mean(hit_rates)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(f\"Top-{K} Recommendations Validation for {sample_user_count} Users:\")\n",
    "print(table)\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies recommended in {K} recs: {avg_hit_rate:.3f}\")\n",
    "print(f\"Precision in {K} recs: {avg_precision:.3f}\")\n",
    "print(f\"Recall in {K} recs: {avg_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21fe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing genres\n",
    "import pandas as pd\n",
    "\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "\n",
    "all_genres = sorted({g for genres in movies['genre_list'] for g in genres})\n",
    "\n",
    "# One-hot encode genres\n",
    "for g in all_genres:\n",
    "    movies[g] = movies['genre_list'].apply(lambda x: int(g in x))\n",
    "\n",
    "filtered_data_genres = filtered_data.merge(movies[['movieId'] + all_genres], on='movieId', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum ratings thresholds to make data manageable\n",
    "\n",
    "min_movie_ratings = 20\n",
    "min_user_ratings = 5\n",
    "\n",
    "# Filter movies\n",
    "movie_counts = filtered_data_genres.groupby('movieId')['rating'].count()\n",
    "movies_to_keep = movie_counts[movie_counts >= min_movie_ratings].index\n",
    "filtered_data_genres = filtered_data_genres[filtered_data_genres['movieId'].isin(movies_to_keep)]\n",
    "\n",
    "# Filter users\n",
    "user_counts = filtered_data_genres.groupby('userId')['rating'].count()\n",
    "users_to_keep = user_counts[user_counts >= min_user_ratings].index\n",
    "filtered_data_genres = filtered_data_genres[filtered_data_genres['userId'].isin(users_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre-based Recommendations (20/80 train/test split):\n",
      "             Recommended  Not Recommended         Total\n",
      "Liked           0.029442        11.292386     11.321827\n",
      "Not Liked       0.023350         9.823350      9.846701\n",
      "Not Watched     9.947208     23122.884264  23132.831472\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies recommended in 10 recs: 0.004\n",
      "Precision in 10 recs: 0.003\n",
      "Recall in 10 recs: 0.004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "K = 10\n",
    "liked_threshold = 4.0\n",
    "test_fraction = 0.2\n",
    "n_jobs = -1\n",
    "n_users_debug = 1000  # set number of users to debug \n",
    "\n",
    "np.random.seed(123)\n",
    "all_user_ids = filtered_data_genres['userId'].unique()\n",
    "sample_user_ids = np.random.choice(all_user_ids, min(n_users_debug, len(all_user_ids)), replace=False)\n",
    "\n",
    "# Filter dataset for these users\n",
    "data_subset = filtered_data_genres[filtered_data_genres['userId'].isin(sample_user_ids)].copy()\n",
    "\n",
    "# --------------------------\n",
    "# Build movie-genre sparse matrix\n",
    "# --------------------------\n",
    "genre_columns = all_genres \n",
    "unique_movies = filtered_data_genres.drop_duplicates('movieId').set_index('movieId')\n",
    "movie_ids = unique_movies.index.values\n",
    "movie_genre_sparse = csr_matrix(unique_movies[genre_columns].values)\n",
    "movie_genre_sparse = normalize(movie_genre_sparse, axis=1)\n",
    "movieid_to_idx = {mid: i for i, mid in enumerate(movie_ids)}\n",
    "\n",
    "# --------------------------\n",
    "# Train/test split per user\n",
    "# --------------------------\n",
    "train_data = []\n",
    "test_data = []\n",
    "for user_id, group in data_subset.groupby('userId'):\n",
    "    group = group.sample(frac=1, random_state=123)  # shuffle\n",
    "    split_idx = int(len(group) * (1 - test_fraction))\n",
    "    train_data.append(group.iloc[:split_idx])\n",
    "    test_data.append(group.iloc[split_idx:])\n",
    "\n",
    "train_data = pd.concat(train_data)\n",
    "test_data = pd.concat(test_data)\n",
    "\n",
    "# --------------------------\n",
    "# Build user profiles from training data\n",
    "# --------------------------\n",
    "user_profiles = {}\n",
    "user_rated_train = {}\n",
    "user_test_liked = {}\n",
    "user_test_not_liked = {}\n",
    "\n",
    "for user_id, group in train_data.groupby('userId'):\n",
    "    liked = group[group['rating'] >= liked_threshold]\n",
    "    not_liked = group[group['rating'] < liked_threshold]\n",
    "\n",
    "    if liked.empty:\n",
    "        continue\n",
    "\n",
    "    liked_indices = [movieid_to_idx[mid] for mid in liked['movieId'] if mid in movieid_to_idx]\n",
    "    if not liked_indices:\n",
    "        continue\n",
    "\n",
    "    profile_vector = movie_genre_sparse[liked_indices].mean(axis=0)\n",
    "    profile_vector = np.asarray(profile_vector)  # convert from np.matrix\n",
    "    profile_vector = normalize(profile_vector)  # normalize\n",
    "    user_profiles[user_id] = profile_vector.ravel()\n",
    "    user_rated_train[user_id] = set(group['movieId'])\n",
    "\n",
    "# --------------------------\n",
    "# Recommendation function\n",
    "# --------------------------\n",
    "def recommend_for_user(user_id, profile_vector, user_rated_train, movie_ids, movie_genre_sparse, K=10):\n",
    "    rated_movies = user_rated_train.get(user_id, set())\n",
    "    candidate_ids = np.array([mid for mid in movie_ids if mid not in rated_movies])\n",
    "    if len(candidate_ids) == 0:\n",
    "        return {'userId': user_id, 'recommended': set()}\n",
    "    candidate_indices = [movieid_to_idx[mid] for mid in candidate_ids]\n",
    "    sims = np.asarray(movie_genre_sparse[candidate_indices].dot(profile_vector.T)).ravel()\n",
    "    top_idx = np.argsort(-sims)[:min(K, len(candidate_ids))]\n",
    "    recommended = set(candidate_ids[top_idx])\n",
    "    return {'userId': user_id, 'recommended': recommended}\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(recommend_for_user)(\n",
    "        user_id, profile, user_rated_train, movie_ids, movie_genre_sparse, K\n",
    "    )\n",
    "    for user_id, profile in user_profiles.items()\n",
    ")\n",
    "\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "for res in results:\n",
    "    user_id = res['userId']\n",
    "    recommended = res['recommended']\n",
    "\n",
    "    user_test = test_data[test_data['userId'] == user_id]\n",
    "    test_liked = set(user_test[user_test['rating'] >= liked_threshold]['movieId'])\n",
    "    test_not_liked = set(user_test[user_test['rating'] < liked_threshold]['movieId'])\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = set(movie_ids) - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    hit_rates.append(len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "    precisions.append(len(recommended & test_liked) / len(recommended) if len(recommended) > 0 else 0)\n",
    "    recalls.append(len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "\n",
    "# Table\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "average_counts = counts_df.mean()\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        average_counts['Recommended_Watched+Liked'],\n",
    "        average_counts['Recommended_Watched+NotLiked'],\n",
    "        average_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        average_counts['NotRecommended_Watched+Liked'],\n",
    "        average_counts['NotRecommended_Watched+NotLiked'],\n",
    "        average_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "print(\"Genre-based Recommendations (20/80 train/test split):\")\n",
    "print(table)\n",
    "\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies recommended in {K} recs: {np.mean(hit_rates):.3f}\")\n",
    "print(f\"Precision in {K} recs: {np.mean(precisions):.3f}\")\n",
    "print(f\"Recall in {K} recs: {np.mean(recalls):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12998e1b",
   "metadata": {},
   "source": [
    "Some thoughts after this\n",
    "- Genre based seems to be the worst in both absolute number of liked movies and ratio with not liked.\n",
    "- Top 10 movies used for recommendation have a good ratio of like to not liked, but this is expected from the highest rated movies in the dataset.\n",
    "- User similarity has the highest absolute value of liked movies from the recommendations, indiciating that it is often the case that people actually watch the movies that were recommended to them.\n",
    "- Movies that are unwatched make it more difficult to interpret recommendations, as it is unknown if the user would like this recommendation or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
