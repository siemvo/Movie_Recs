{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b09f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vonks\\Documents\\GitHub\\Movie_Recs\\Downloads\n"
     ]
    }
   ],
   "source": [
    "# Data from https://grouplens.org/datasets/movielens/latest/\n",
    "# Put the downloaded csv files in the \"Downloads\" folder\n",
    "\n",
    "import os\n",
    "os.chdir(\"Downloads\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366cf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "data = ratings.merge(movies, on=\"movieId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a84fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing users with more than 1.000 ratings as these are likely to be bots or outliers\n",
    "# See Data_Exploration.ipynb for more details about cleaning and outlier detection\n",
    "\n",
    "ratings_per_user = data.groupby('userId').size()\n",
    "outliers = ratings_per_user[ratings_per_user > 1000] # or 10.000\n",
    "filtered_data = data[~data['userId'].isin(outliers.index)]\n",
    "\n",
    "# Table of removed users\n",
    "removed_users_table = outliers.reset_index()\n",
    "removed_users_table.columns = ['userId', 'num_ratings']\n",
    "\n",
    "#print(removed_users_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340b0b4",
   "metadata": {},
   "source": [
    "## User Similarity Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47cb32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Parameters\n",
    "min_ratings_per_user = 20 # For selection of active users\n",
    "sample_size = 100\n",
    "K = 10 # Number of recommendations\n",
    "liked_threshold = 4\n",
    "top_n_input = 4 # Number of liked movies to use for similarity\n",
    "\n",
    "# Select active users\n",
    "liked_mask = filtered_data['rating'] >= liked_threshold\n",
    "liked_counts = filtered_data[liked_mask].groupby('userId').size()\n",
    "active_users = filtered_data.groupby('userId').size()\n",
    "active_users = active_users[(active_users >= min_ratings_per_user)].index\n",
    "users_with_5_liked = liked_counts[liked_counts >= 5].index\n",
    "active_users = active_users.intersection(users_with_5_liked)\n",
    "\n",
    "# Random sample of users because data is large\n",
    "np.random.seed(123)\n",
    "sampled_users = np.random.choice(active_users, size=sample_size, replace=False)\n",
    "\n",
    "# For each sampled user, split their movie ratings into train (80%) and test (20%)\n",
    "train_ratings = pd.DataFrame()\n",
    "test_ratings = pd.DataFrame()\n",
    "\n",
    "for uid in sampled_users:\n",
    "    user_data = filtered_data[filtered_data['userId'] == uid]\n",
    "    train, test = train_test_split(user_data, test_size=0.2, random_state=123)\n",
    "    train_ratings = pd.concat([train_ratings, train])\n",
    "    test_ratings = pd.concat([test_ratings, test])\n",
    "\n",
    "# -----------------------------\n",
    "# Build matrix from training data\n",
    "# -----------------------------\n",
    "user_ids = train_ratings['userId'].unique()\n",
    "movie_ids = train_ratings['movieId'].unique()\n",
    "user_to_idx = {uid:i for i, uid in enumerate(user_ids)} # Rows\n",
    "movie_to_idx = {mid:i for i, mid in enumerate(movie_ids)} # Columns\n",
    "\n",
    "# Empty matrix\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "user_item_matrix = np.zeros((n_users, n_movies))\n",
    "\n",
    "# Fill matrix\n",
    "for row in train_ratings.itertuples():\n",
    "    u_idx = user_to_idx[row.userId]\n",
    "    m_idx = movie_to_idx[row.movieId]\n",
    "    user_item_matrix[u_idx, m_idx] = row.rating\n",
    "\n",
    "# -----------------------------\n",
    "# Generate recommendations using only top liked movies\n",
    "# -----------------------------\n",
    "user_metrics = []\n",
    "\n",
    "for uid in sampled_users:\n",
    "    u_idx = user_to_idx[uid]\n",
    "    user_vector = user_item_matrix[u_idx, :] # Ratings\n",
    "\n",
    "    # Select top-N liked movies only (positive ratings)\n",
    "    liked_indices = np.where(user_vector >= liked_threshold)[0]\n",
    "    if len(liked_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    top_indices = np.argsort(user_vector[liked_indices])[-top_n_input:]\n",
    "    top_indices = liked_indices[top_indices]\n",
    "\n",
    "    # Find relevant users who rated at least one of these top-N movies\n",
    "    rel_users = np.any(user_item_matrix[:, top_indices] > 0, axis=1)\n",
    "    rel_users[u_idx] = False\n",
    "    relevant_users_matrix = user_item_matrix[rel_users, :]\n",
    "\n",
    "    if relevant_users_matrix.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # Cosine similarity on top-N liked movies\n",
    "    sims = cosine_similarity([user_vector[top_indices]], relevant_users_matrix[:, top_indices])[0]\n",
    "\n",
    "    # Compute weighted average for unseen movies based on similar users = prediction\n",
    "    unseen = np.where(user_vector == 0)[0]\n",
    "    weighted_scores = np.zeros(len(unseen))\n",
    "    for i, m_idx in enumerate(unseen):\n",
    "        weighted_scores[i] = np.sum(relevant_users_matrix[:, m_idx] * sims) / (np.sum(sims) + 1e-8)\n",
    "\n",
    "    # Top recommendations\n",
    "    top_recs_idx = np.argsort(weighted_scores)[::-1][:K] # Top K\n",
    "    recommended_movie_ids = [list(movie_to_idx.keys())[unseen[i]] for i in top_recs_idx]\n",
    "\n",
    "    # Test set \n",
    "    test_all = test_ratings[test_ratings['userId'] == uid]\n",
    "    test_liked = set(test_all[test_all['rating'] >= liked_threshold]['movieId'])\n",
    "    test_not_liked = set(test_all[test_all['rating'] < liked_threshold]['movieId'])\n",
    "\n",
    "    user_metrics.append({\n",
    "        'recommended': recommended_movie_ids,\n",
    "        'test_liked': test_liked,\n",
    "        'test_not_liked': test_not_liked\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30a6cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations based on user similarity of liked movies:\n",
      "             Recommended  Not Recommended         Total\n",
      "Liked           1.305263        15.000000     16.305263\n",
      "Not Liked       0.515789        16.810526     17.326316\n",
      "Not Watched     8.178947     59433.189474  59441.368421\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies in 10 recs: 0.099\n",
      "Precision in 10 recs: 0.131\n",
      "Recall in 10 recs: 0.099\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "all_movie_ids = set(filtered_data['movieId'].unique())\n",
    "K = 10  # top-K recommendations\n",
    "liked_threshold = 4.0\n",
    "\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "#filtered_user_metrics = [um for um in user_metrics if len(um['test_liked']) > 0]\n",
    "\n",
    "for um in user_metrics:\n",
    "    # Unpack the information per user (um)\n",
    "    recommended = set(um['recommended'])\n",
    "    test_liked = um['test_liked']\n",
    "    test_not_liked = um['test_not_liked']\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    # Metrics for table\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = all_movie_ids - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    # Hit Rate = 1 if at least one liked movie is recommended\n",
    "    #hit = 1 if len(recommended & test_liked) > 0 else 0\n",
    "    hit = len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0\n",
    "\n",
    "    # Precision and Recall K\n",
    "    precision = len(recommended & test_liked) / len(recommended) if len(recommended) > 0 else 0\n",
    "    recall = len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0\n",
    "    \n",
    "    hit_rates.append(hit)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Table with averages\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "average_counts = counts_df.mean()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        average_counts['Recommended_Watched+Liked'],\n",
    "        average_counts['Recommended_Watched+NotLiked'],\n",
    "        average_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        average_counts['NotRecommended_Watched+Liked'],\n",
    "        average_counts['NotRecommended_Watched+NotLiked'],\n",
    "        average_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "\n",
    "# Add totals column\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "print(\"Recommendations based on user similarity of liked movies:\")\n",
    "print(table)\n",
    "\n",
    "avg_hit_rate = np.mean(hit_rates)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies in {K} recs: {avg_hit_rate:.3f}\")\n",
    "print(f\"Precision in {K} recs: {avg_precision:.3f}\")\n",
    "print(f\"Recall in {K} recs: {avg_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066577d",
   "metadata": {},
   "source": [
    "## Recommending based on top 10 rated movies overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139889ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Movies in the Entire Dataset (min 10000 ratings):\n",
      "   movieId                                              title  average_rating  \\\n",
      "0      318                   Shawshank Redemption, The (1994)        4.419662   \n",
      "1   202439                                    Parasite (2019)        4.353265   \n",
      "2      858                              Godfather, The (1972)        4.326352   \n",
      "3       50                         Usual Suspects, The (1995)        4.274643   \n",
      "4     1221                     Godfather: Part II, The (1974)        4.273383   \n",
      "5     1203                                12 Angry Men (1957)        4.273294   \n",
      "6     2019        Seven Samurai (Shichinin no samurai) (1954)        4.261119   \n",
      "7      527                            Schindler's List (1993)        4.246293   \n",
      "8     5618  Spirited Away (Sen to Chihiro no kamikakushi) ...        4.240083   \n",
      "9     2959                                  Fight Club (1999)        4.239283   \n",
      "\n",
      "   num_ratings  \n",
      "0       118898  \n",
      "1        11163  \n",
      "2        71881  \n",
      "3        69716  \n",
      "4        44520  \n",
      "5        20542  \n",
      "6        15334  \n",
      "7        81267  \n",
      "8        33151  \n",
      "9        82858  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "K = 10\n",
    "min_ratings_per_movie = 10000  # threshold applied on the ENTIRE dataset\n",
    "\n",
    "movie_stats_all = filtered_data.groupby('movieId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "eligible_movies = movie_stats_all[movie_stats_all['count'] >= min_ratings_per_movie]\n",
    "\n",
    "# Sort by average rating descending\n",
    "top_movies_by_avg_all = eligible_movies.sort_values('mean', ascending=False).head(K)\n",
    "\n",
    "# Merge with movie titles\n",
    "top_movies_summary = top_movies_by_avg_all.merge(movies[['movieId', 'title']], on='movieId', how='left')\n",
    "\n",
    "# Rename columns\n",
    "top_movies_summary.rename(columns={'mean': 'average_rating', 'count': 'num_ratings'}, inplace=True)\n",
    "top_movies_summary = top_movies_summary[['movieId', 'title', 'average_rating', 'num_ratings']]\n",
    "\n",
    "# Display top movies\n",
    "print(f\"Top-{K} Movies in the Entire Dataset (min {min_ratings_per_movie} ratings):\")\n",
    "print(top_movies_summary)\n",
    "\n",
    "# Save movie IDs for later recommendation block\n",
    "top_movie_ids = top_movies_summary['movieId'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1548f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Recommendations Validation for 100 Users:\n",
      "             Recommended  Not Recommended     Total\n",
      "Liked               0.51            16.04     16.55\n",
      "Not Liked           0.11            17.78     17.89\n",
      "Not Watched         9.38         59431.18  59440.56\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies in 10 recs: 0.034\n",
      "Precision in 10 recs: 0.051\n",
      "Recall in 10 recs: 0.034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# Parameters (consistent with block 1)\n",
    "# --------------------------\n",
    "liked_threshold = 4\n",
    "K = 10              # number of recommendations\n",
    "top_n_input = 4     # top liked movies used as input in block 1\n",
    "all_movie_ids = set(filtered_data['movieId'].unique())\n",
    "\n",
    "user_metrics_top = []\n",
    "\n",
    "# --------------------------\n",
    "# Build per-user evaluation\n",
    "# --------------------------\n",
    "for uid in sampled_users:\n",
    "    # Test set for this user\n",
    "    test_all = test_ratings[test_ratings['userId'] == uid]\n",
    "\n",
    "    # Top-N liked movies from TRAIN (same as block 1)\n",
    "    train_user = train_ratings[train_ratings['userId'] == uid]\n",
    "    liked_train = train_user[train_user['rating'] >= liked_threshold]\n",
    "    if liked_train.empty:\n",
    "        continue\n",
    "\n",
    "    top_liked_train = liked_train.sort_values('rating', ascending=False).head(top_n_input)['movieId'].tolist()\n",
    "\n",
    "    # Recommendations for this user (baseline: top_movie_ids)\n",
    "    recommended = set(top_movie_ids)\n",
    "\n",
    "    # Exclude top-N liked movies from test liked set\n",
    "    test_liked = set(test_all[test_all['rating'] >= liked_threshold]['movieId']) - set(top_liked_train)\n",
    "    test_not_liked = set(test_all[test_all['rating'] < liked_threshold]['movieId'])\n",
    "\n",
    "    user_metrics_top.append({\n",
    "        'recommended': recommended,\n",
    "        'test_liked': test_liked,\n",
    "        'test_not_liked': test_not_liked\n",
    "    })\n",
    "\n",
    "# --------------------------\n",
    "# Compute counts and evaluation metrics\n",
    "# --------------------------\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "for um in user_metrics_top:\n",
    "    recommended = um['recommended']\n",
    "    test_liked = um['test_liked']\n",
    "    test_not_liked = um['test_not_liked']\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = all_movie_ids - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    # Hit rate, precision, recall\n",
    "    hit_rates.append(rec_watched_liked / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "    precisions.append(rec_watched_liked / K)\n",
    "    recalls.append(rec_watched_liked / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "\n",
    "# --------------------------\n",
    "# Build table\n",
    "# --------------------------\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "avg_counts = counts_df.mean()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        avg_counts['Recommended_Watched+Liked'],\n",
    "        avg_counts['Recommended_Watched+NotLiked'],\n",
    "        avg_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        avg_counts['NotRecommended_Watched+Liked'],\n",
    "        avg_counts['NotRecommended_Watched+NotLiked'],\n",
    "        avg_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "avg_hit_rate = np.mean(hit_rates)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(f\"Top-{K} Recommendations Validation for {len(sampled_users)} Users:\")\n",
    "print(table)\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies in {K} recs: {avg_hit_rate:.3f}\")\n",
    "print(f\"Precision in {K} recs: {avg_precision:.3f}\")\n",
    "print(f\"Recall in {K} recs: {avg_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ab9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Recommendations Validation for 100 Users:\n",
      "             Recommended  Not Recommended     Total\n",
      "Liked               0.50            15.38     15.88\n",
      "Not Liked           0.08            17.66     17.74\n",
      "Not Watched         9.42         59431.96  59441.38\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies in 10 recs: 0.038\n",
      "Precision in 10 recs: 0.050\n",
      "Recall in 10 recs: 0.038\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# Parameters (consistent with block 1)\n",
    "# --------------------------\n",
    "liked_threshold = 4\n",
    "K = 10              # same as in block 1\n",
    "top_n_input = 4     # same as in block 1\n",
    "all_movie_ids = set(filtered_data['movieId'].unique())\n",
    "\n",
    "user_metrics_top = []\n",
    "\n",
    "for uid in sampled_users:\n",
    "    # Test set for this user\n",
    "    test_all = test_ratings[test_ratings['userId'] == uid]\n",
    "\n",
    "    # Find top-N liked movies from TRAIN (same as block 1)\n",
    "    train_user = train_ratings[train_ratings['userId'] == uid]\n",
    "    liked_train = train_user[train_user['rating'] >= liked_threshold]\n",
    "    if liked_train.empty:\n",
    "        continue\n",
    "\n",
    "    top_liked_train = liked_train.sort_values('rating', ascending=False).head(top_n_input)['movieId'].tolist()\n",
    "\n",
    "    # Recommendations (provided externally, e.g. popular or top_movie_ids)\n",
    "    recommended = set(top_movie_ids)\n",
    "\n",
    "    # Exclude top-N liked movies (since they were used as input)\n",
    "    test_liked = set(test_all[test_all['rating'] >= liked_threshold]['movieId']) - set(top_liked_train)\n",
    "    test_not_liked = set(test_all[test_all['rating'] < liked_threshold]['movieId'])\n",
    "\n",
    "    user_metrics_top.append({\n",
    "        'recommended': recommended,\n",
    "        'test_liked': test_liked,\n",
    "        'test_not_liked': test_not_liked\n",
    "    })\n",
    "\n",
    "# --------------------------\n",
    "# Compute counts and evaluation metrics\n",
    "# --------------------------\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "for um in user_metrics_top:\n",
    "    recommended = um['recommended']\n",
    "    test_liked = um['test_liked']\n",
    "    test_not_liked = um['test_not_liked']\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = all_movie_ids - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    # Hit rate, precision, recall\n",
    "    hit_rates.append(rec_watched_liked / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "    precisions.append(rec_watched_liked / K)\n",
    "    recalls.append(rec_watched_liked / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "\n",
    "# --------------------------\n",
    "# Table\n",
    "# --------------------------\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "avg_counts = counts_df.mean()\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        avg_counts['Recommended_Watched+Liked'],\n",
    "        avg_counts['Recommended_Watched+NotLiked'],\n",
    "        avg_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        avg_counts['NotRecommended_Watched+Liked'],\n",
    "        avg_counts['NotRecommended_Watched+NotLiked'],\n",
    "        avg_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "avg_hit_rate = np.mean(hit_rates)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "\n",
    "print(f\"Top-{K} Recommendations Validation for {len(sampled_users)} Users:\")\n",
    "print(table)\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies in {K} recs: {avg_hit_rate:.3f}\")\n",
    "print(f\"Precision in {K} recs: {avg_precision:.3f}\")\n",
    "print(f\"Recall in {K} recs: {avg_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21fe407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing genres\n",
    "import pandas as pd\n",
    "\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "\n",
    "all_genres = sorted({g for genres in movies['genre_list'] for g in genres})\n",
    "\n",
    "# One-hot encode genres\n",
    "for g in all_genres:\n",
    "    movies[g] = movies['genre_list'].apply(lambda x: int(g in x))\n",
    "\n",
    "filtered_data_genres = filtered_data.merge(movies[['movieId'] + all_genres], on='movieId', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb3d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum ratings thresholds to make data manageable\n",
    "\n",
    "min_movie_ratings = 20\n",
    "min_user_ratings = 5\n",
    "\n",
    "# Filter movies\n",
    "movie_counts = filtered_data_genres.groupby('movieId')['rating'].count()\n",
    "movies_to_keep = movie_counts[movie_counts >= min_movie_ratings].index\n",
    "filtered_data_genres = filtered_data_genres[filtered_data_genres['movieId'].isin(movies_to_keep)]\n",
    "\n",
    "# Filter users\n",
    "user_counts = filtered_data_genres.groupby('userId')['rating'].count()\n",
    "users_to_keep = user_counts[user_counts >= min_user_ratings].index\n",
    "filtered_data_genres = filtered_data_genres[filtered_data_genres['userId'].isin(users_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e5b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre-based Recommendations (20/80 train/test split):\n",
      "             Recommended  Not Recommended         Total\n",
      "Liked           0.033469         9.629817      9.663286\n",
      "Not Liked       0.025355         8.492901      8.518256\n",
      "Not Watched     9.941176     16657.877282  16667.818458\n",
      "\n",
      "Average metrics:\n",
      "Fraction of liked movies recommended in 10 recs: 0.006\n",
      "Precision in 10 recs: 0.003\n",
      "Recall in 10 recs: 0.006\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "K = 10\n",
    "liked_threshold = 4.0\n",
    "test_fraction = 0.2\n",
    "n_jobs = -1\n",
    "n_users_debug = 1000  # set number of users to debug \n",
    "\n",
    "np.random.seed(123)\n",
    "all_user_ids = filtered_data_genres['userId'].unique()\n",
    "sample_user_ids = np.random.choice(all_user_ids, min(n_users_debug, len(all_user_ids)), replace=False)\n",
    "\n",
    "# Filter dataset for these users\n",
    "data_subset = filtered_data_genres[filtered_data_genres['userId'].isin(sample_user_ids)].copy()\n",
    "\n",
    "# --------------------------\n",
    "# Build movie-genre sparse matrix\n",
    "# --------------------------\n",
    "genre_columns = all_genres \n",
    "unique_movies = filtered_data_genres.drop_duplicates('movieId').set_index('movieId')\n",
    "movie_ids = unique_movies.index.values\n",
    "movie_genre_sparse = csr_matrix(unique_movies[genre_columns].values)\n",
    "movie_genre_sparse = normalize(movie_genre_sparse, axis=1)\n",
    "movieid_to_idx = {mid: i for i, mid in enumerate(movie_ids)}\n",
    "\n",
    "# --------------------------\n",
    "# Train/test split per user\n",
    "# --------------------------\n",
    "train_data = []\n",
    "test_data = []\n",
    "for user_id, group in data_subset.groupby('userId'):\n",
    "    group = group.sample(frac=1, random_state=123)  # shuffle\n",
    "    split_idx = int(len(group) * (1 - test_fraction))\n",
    "    train_data.append(group.iloc[:split_idx])\n",
    "    test_data.append(group.iloc[split_idx:])\n",
    "\n",
    "train_data = pd.concat(train_data)\n",
    "test_data = pd.concat(test_data)\n",
    "\n",
    "# --------------------------\n",
    "# Build user profiles from training data\n",
    "# --------------------------\n",
    "user_profiles = {}\n",
    "user_rated_train = {}\n",
    "user_test_liked = {}\n",
    "user_test_not_liked = {}\n",
    "\n",
    "for user_id, group in train_data.groupby('userId'):\n",
    "    liked = group[group['rating'] >= liked_threshold]\n",
    "    not_liked = group[group['rating'] < liked_threshold]\n",
    "\n",
    "    if liked.empty:\n",
    "        continue\n",
    "\n",
    "    liked_indices = [movieid_to_idx[mid] for mid in liked['movieId'] if mid in movieid_to_idx]\n",
    "    if not liked_indices:\n",
    "        continue\n",
    "\n",
    "    profile_vector = movie_genre_sparse[liked_indices].mean(axis=0)\n",
    "    profile_vector = np.asarray(profile_vector)  # convert from np.matrix\n",
    "    profile_vector = normalize(profile_vector)  # normalize\n",
    "    user_profiles[user_id] = profile_vector.ravel()\n",
    "    user_rated_train[user_id] = set(group['movieId'])\n",
    "\n",
    "# --------------------------\n",
    "# Recommendation function\n",
    "# --------------------------\n",
    "def recommend_for_user(user_id, profile_vector, user_rated_train, movie_ids, movie_genre_sparse, K=10):\n",
    "    rated_movies = user_rated_train.get(user_id, set())\n",
    "    candidate_ids = np.array([mid for mid in movie_ids if mid not in rated_movies])\n",
    "    if len(candidate_ids) == 0:\n",
    "        return {'userId': user_id, 'recommended': set()}\n",
    "    candidate_indices = [movieid_to_idx[mid] for mid in candidate_ids]\n",
    "    sims = np.asarray(movie_genre_sparse[candidate_indices].dot(profile_vector.T)).ravel()\n",
    "    top_idx = np.argsort(-sims)[:min(K, len(candidate_ids))]\n",
    "    recommended = set(candidate_ids[top_idx])\n",
    "    return {'userId': user_id, 'recommended': recommended}\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(recommend_for_user)(\n",
    "        user_id, profile, user_rated_train, movie_ids, movie_genre_sparse, K\n",
    "    )\n",
    "    for user_id, profile in user_profiles.items()\n",
    ")\n",
    "\n",
    "counts_per_user = []\n",
    "hit_rates, precisions, recalls = [], [], []\n",
    "\n",
    "for res in results:\n",
    "    user_id = res['userId']\n",
    "    recommended = res['recommended']\n",
    "\n",
    "    user_test = test_data[test_data['userId'] == user_id]\n",
    "    test_liked = set(user_test[user_test['rating'] >= liked_threshold]['movieId'])\n",
    "    test_not_liked = set(user_test[user_test['rating'] < liked_threshold]['movieId'])\n",
    "    test_movies = test_liked | test_not_liked\n",
    "\n",
    "    rec_watched_liked = len(recommended & test_liked)\n",
    "    rec_watched_not_liked = len(recommended & test_not_liked)\n",
    "    rec_not_watched = len(recommended - test_movies)\n",
    "\n",
    "    nonrec = set(movie_ids) - recommended\n",
    "    nonrec_watched_liked = len(nonrec & test_liked)\n",
    "    nonrec_watched_not_liked = len(nonrec & test_not_liked)\n",
    "    nonrec_not_watched = len(nonrec - test_movies)\n",
    "\n",
    "    counts_per_user.append({\n",
    "        'Recommended_Watched+Liked': rec_watched_liked,\n",
    "        'Recommended_Watched+NotLiked': rec_watched_not_liked,\n",
    "        'Recommended_NotWatched': rec_not_watched,\n",
    "        'NotRecommended_Watched+Liked': nonrec_watched_liked,\n",
    "        'NotRecommended_Watched+NotLiked': nonrec_watched_not_liked,\n",
    "        'NotRecommended_NotWatched': nonrec_not_watched\n",
    "    })\n",
    "\n",
    "    hit_rates.append(len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "    precisions.append(len(recommended & test_liked) / len(recommended) if len(recommended) > 0 else 0)\n",
    "    recalls.append(len(recommended & test_liked) / len(test_liked) if len(test_liked) > 0 else 0)\n",
    "\n",
    "# Table\n",
    "counts_df = pd.DataFrame(counts_per_user)\n",
    "average_counts = counts_df.mean()\n",
    "table = pd.DataFrame({\n",
    "    'Recommended': [\n",
    "        average_counts['Recommended_Watched+Liked'],\n",
    "        average_counts['Recommended_Watched+NotLiked'],\n",
    "        average_counts['Recommended_NotWatched']\n",
    "    ],\n",
    "    'Not Recommended': [\n",
    "        average_counts['NotRecommended_Watched+Liked'],\n",
    "        average_counts['NotRecommended_Watched+NotLiked'],\n",
    "        average_counts['NotRecommended_NotWatched']\n",
    "    ]\n",
    "}, index=['Liked', 'Not Liked', 'Not Watched'])\n",
    "table['Total'] = table['Recommended'] + table['Not Recommended']\n",
    "\n",
    "print(\"Genre-based Recommendations (20/80 train/test split):\")\n",
    "print(table)\n",
    "\n",
    "print(\"\\nAverage metrics:\")\n",
    "print(f\"Fraction of liked movies recommended in {K} recs: {np.mean(hit_rates):.3f}\")\n",
    "print(f\"Precision in {K} recs: {np.mean(precisions):.3f}\")\n",
    "print(f\"Recall in {K} recs: {np.mean(recalls):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12998e1b",
   "metadata": {},
   "source": [
    "Some thoughts after this\n",
    "- Genre based seems to be the worst in both absolute number of liked movies and ratio with not liked.\n",
    "- Top 10 movies used for recommendation have a good ratio of like to not liked, but this is expected from the highest rated movies in the dataset.\n",
    "- User similarity has the highest absolute value of liked movies from the recommendations, indiciating that it is often the case that people actually watch the movies that were recommended to them.\n",
    "- Movies that are unwatched make it more difficult to interpret recommendations, as it is unknown if the user would like this recommendation or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f7191",
   "metadata": {},
   "source": [
    "## User similarity clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fe6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block A: build sparse user-topN matrix, SVD embeddings, MiniBatchKMeans clustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# -----------------------\n",
    "# Parameters\n",
    "# -----------------------\n",
    "top_n_input = 4             # number of top liked movies used to define user profile\n",
    "liked_threshold = 4.0       # movies >= this considered liked\n",
    "min_ratings_per_user = 20   # already used earlier to sample active users\n",
    "sample_size = None          # all users\n",
    "n_components = 50           # SVD dimension (lower = faster); 50 is a good start\n",
    "n_clusters = 200            # number of clusters to produce (tune)\n",
    "mbk_batch_size = 1024       # minibatch size for KMeans\n",
    "random_state = 123\n",
    "\n",
    "# -----------------------\n",
    "# Select users to use\n",
    "# -----------------------\n",
    "# user_metrics should exist from your earlier pipeline; otherwise build similar list\n",
    "# We'll build sparse profiles from train_ratings (so no test leakage)\n",
    "# Use train_ratings from your earlier code\n",
    "\n",
    "# Choose active users (same logic you used before)\n",
    "active_users = train_ratings.groupby('userId').size()\n",
    "active_users = active_users[active_users >= min_ratings_per_user].index.values\n",
    "\n",
    "if sample_size is not None and sample_size < len(active_users):\n",
    "    np.random.seed(random_state)\n",
    "    sampled_users = np.random.choice(active_users, size=sample_size, replace=False)\n",
    "else:\n",
    "    sampled_users = active_users\n",
    "\n",
    "sampled_users = np.array(sampled_users)  # ensure numpy array\n",
    "n_users = len(sampled_users)\n",
    "print(f\"Using {n_users} users for embedding/clustering\")\n",
    "\n",
    "# -----------------------\n",
    "# Build index maps\n",
    "# -----------------------\n",
    "user_to_pos = {uid: i for i, uid in enumerate(sampled_users)}\n",
    "\n",
    "# We'll only include movies that appear as top-N liked for at least someone.\n",
    "# First pass: collect candidate movieIds\n",
    "from collections import Counter\n",
    "movie_counter = Counter()\n",
    "\n",
    "# For efficiency, build per-user top-N liked movies from train_ratings aggregated by user\n",
    "# Create user -> (movieId, rating) lists from train_ratings\n",
    "train_by_user = train_ratings[train_ratings['userId'].isin(sampled_users)].groupby('userId')\n",
    "\n",
    "for uid, grp in train_by_user:\n",
    "    # select liked movies in this user's training set\n",
    "    liked = grp[grp['rating'] >= liked_threshold]\n",
    "    if liked.empty:\n",
    "        continue\n",
    "    # sort by rating desc then count top_n_input\n",
    "    top = liked.sort_values('rating', ascending=False)['movieId'].values[:top_n_input]\n",
    "    for mid in top:\n",
    "        movie_counter[mid] += 1\n",
    "\n",
    "# Keep movies that appear at least once (optionally threshold by frequency to reduce dims)\n",
    "candidate_movies = np.array([mid for mid, cnt in movie_counter.items() if cnt >= 1])\n",
    "movie_to_pos = {mid: i for i, mid in enumerate(candidate_movies)}\n",
    "n_movies_candidate = len(candidate_movies)\n",
    "print(f\"Candidate movie features from top-N liked across users: {n_movies_candidate}\")\n",
    "\n",
    "# -----------------------\n",
    "# Build sparse user x movie matrix (binary or rating-weighted)\n",
    "# -----------------------\n",
    "rows = []\n",
    "cols = []\n",
    "data_vals = []\n",
    "\n",
    "# second pass to fill sparse matrix\n",
    "for uid, grp in train_by_user:\n",
    "    upos = user_to_pos.get(uid, None)\n",
    "    if upos is None:\n",
    "        continue\n",
    "    liked = grp[grp['rating'] >= liked_threshold]\n",
    "    if liked.empty:\n",
    "        continue\n",
    "    top = liked.sort_values('rating', ascending=False)['movieId'].values[:top_n_input]\n",
    "    for mid in top:\n",
    "        mpos = movie_to_pos.get(mid, None)\n",
    "        if mpos is None:\n",
    "            continue\n",
    "        rows.append(upos)\n",
    "        cols.append(mpos)\n",
    "        # Use rating value (optional) or 1 for binary. Using rating gives more signal:\n",
    "        data_vals.append(1.0)  # use 1.0 for binary; switch to rating if desired\n",
    "\n",
    "# Create CSR matrix\n",
    "if len(rows) == 0:\n",
    "    raise RuntimeError(\"No user top-N liked entries found. Check thresholds or sampled users.\")\n",
    "X_sparse = sparse.csr_matrix((data_vals, (rows, cols)), shape=(n_users, n_movies_candidate), dtype=np.float32)\n",
    "print(\"Built sparse user profile matrix:\", X_sparse.shape, \"nnz=\", X_sparse.nnz)\n",
    "\n",
    "# -----------------------\n",
    "# Dimensionality reduction (TruncatedSVD on sparse)\n",
    "# -----------------------\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "user_embeddings = svd.fit_transform(X_sparse)   # result: (n_users, n_components)\n",
    "explained = svd.explained_variance_ratio_.sum()\n",
    "print(f\"SVD done. Embedding shape: {user_embeddings.shape}. Explained variance_ratio sum: {explained:.3f}\")\n",
    "\n",
    "# Optional: L2-normalize embeddings if you plan to use cosine similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "user_embeddings = normalize(user_embeddings, norm='l2', axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# Clustering (MiniBatchKMeans)\n",
    "# -----------------------\n",
    "mbk = MiniBatchKMeans(n_clusters=n_clusters, batch_size=mbk_batch_size, random_state=random_state)\n",
    "cluster_labels = mbk.fit_predict(user_embeddings)\n",
    "print(\"Clustering done. Number of clusters:\", n_clusters)\n",
    "# cluster assignments: cluster_labels (len n_users)\n",
    "\n",
    "# Save outputs to use later\n",
    "# create dataframe aligning user ids with embeddings and cluster labels\n",
    "user_embed_df = pd.DataFrame({\n",
    "    'userId': sampled_users,\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "# optionally include embedding columns\n",
    "for i in range(user_embeddings.shape[1]):\n",
    "    user_embed_df[f'emb_{i}'] = user_embeddings[:, i]\n",
    "\n",
    "# quick cluster sizes\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(\"Cluster sizes (first 10):\")\n",
    "print(cluster_sizes.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef773d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block B: cluster distances & evaluation (uses outputs from Block A)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# user_embeddings (n_users x n_components), mbk (fitted), cluster_labels from Block A\n",
    "# user_embed_df exists mapping userId -> embedding & cluster\n",
    "\n",
    "# 1) Centroids and pairwise centroid distances (Euclidean)\n",
    "centroids = mbk.cluster_centers_   # shape (n_clusters, n_components)\n",
    "centroid_dists = pairwise_distances(centroids, metric='euclidean')  # (n_clusters,n_clusters)\n",
    "\n",
    "# summarize centroid distances\n",
    "centroid_mean = centroid_dists[np.triu_indices_from(centroid_dists, k=1)].mean()\n",
    "centroid_min = centroid_dists[np.triu_indices_from(centroid_dists, k=1)].min()\n",
    "centroid_max = centroid_dists[np.triu_indices_from(centroid_dists, k=1)].max()\n",
    "print(f\"Centroid pairwise distances (eucl): mean={centroid_mean:.3f}, min={centroid_min:.3f}, max={centroid_max:.3f}\")\n",
    "\n",
    "# 2) Average intra-cluster cosine similarity and inter-cluster cosine similarity\n",
    "# compute user embeddings grouped by cluster (sample clusters if too many)\n",
    "from collections import defaultdict\n",
    "emb_by_cluster = defaultdict(list)\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    emb_by_cluster[c].append(user_embeddings[i])\n",
    "\n",
    "intra_sims = []\n",
    "inter_sims = []\n",
    "\n",
    "# to keep fast, sample up to max_per_cluster embeddings per cluster\n",
    "max_per_cluster = 200\n",
    "for c, embs in emb_by_cluster.items():\n",
    "    embs = np.asarray(embs)\n",
    "    m = embs.shape[0]\n",
    "    if m <= 1:\n",
    "        continue\n",
    "    idxs = np.random.choice(m, size=min(m, max_per_cluster), replace=False)\n",
    "    sample_embs = embs[idxs]\n",
    "    # intra-cluster pairwise cosine\n",
    "    sims = cosine_similarity(sample_embs)\n",
    "    # take upper triangle mean excluding diagonal\n",
    "    iu = np.triu_indices_from(sims, k=1)\n",
    "    intra_sims.append(np.mean(sims[iu]))\n",
    "\n",
    "# For inter-cluster similarity: sample embeddings from different clusters\n",
    "# pick many pairs of clusters and compute mean cross-similarity\n",
    "cluster_ids = list(emb_by_cluster.keys())\n",
    "n_pair_samples = 200\n",
    "for _ in range(n_pair_samples):\n",
    "    a, b = np.random.choice(cluster_ids, size=2, replace=False)\n",
    "    a_embs = np.asarray(emb_by_cluster[a])\n",
    "    b_embs = np.asarray(emb_by_cluster[b])\n",
    "    if a_embs.shape[0] == 0 or b_embs.shape[0] == 0:\n",
    "        continue\n",
    "    a_idx = np.random.choice(a_embs.shape[0], size=min(max_per_cluster, a_embs.shape[0]), replace=False)\n",
    "    b_idx = np.random.choice(b_embs.shape[0], size=min(max_per_cluster, b_embs.shape[0]), replace=False)\n",
    "    sims = cosine_similarity(a_embs[a_idx], b_embs[b_idx])\n",
    "    inter_sims.append(np.mean(sims))\n",
    "\n",
    "print(f\"Avg intra-cluster cosine sim: {np.mean(intra_sims):.4f} (std {np.std(intra_sims):.4f})\")\n",
    "print(f\"Avg inter-cluster cosine sim: {np.mean(inter_sims):.4f} (std {np.std(inter_sims):.4f})\")\n",
    "\n",
    "# 3) Silhouette score (compute on a subsample to save time)\n",
    "# silhouette uses distance metric; for large n_users compute on a sample\n",
    "max_sil_sample = 20000\n",
    "if user_embeddings.shape[0] > max_sil_sample:\n",
    "    idx = np.random.choice(user_embeddings.shape[0], size=max_sil_sample, replace=False)\n",
    "    sil = silhouette_score(user_embeddings[idx], cluster_labels[idx], metric='cosine')\n",
    "else:\n",
    "    sil = silhouette_score(user_embeddings, cluster_labels, metric='cosine')\n",
    "print(f\"Silhouette score (cosine) on sample: {sil:.4f}\")\n",
    "\n",
    "# 4) How well \"top-4 similarity\" differentiates users:\n",
    "# For each user, compute how many users share at least 1 or >=2 same top-N movies and check same-cluster rate.\n",
    "\n",
    "# Build map user -> set(topN movieIds)\n",
    "user_topn = {}\n",
    "train_by_user = train_ratings[train_ratings['userId'].isin(sampled_users)].groupby('userId')\n",
    "for uid, grp in train_by_user:\n",
    "    liked = grp[grp['rating'] >= liked_threshold]\n",
    "    top = liked.sort_values('rating', ascending=False)['movieId'].values[:top_n_input]\n",
    "    user_topn[uid] = set(top)\n",
    "\n",
    "# Create quick lookup: movie -> users who have it in their topN\n",
    "movie_to_users = defaultdict(list)\n",
    "for uid, mids in user_topn.items():\n",
    "    for m in mids:\n",
    "        movie_to_users[m].append(uid)\n",
    "\n",
    "# For each user sample, find \"neighbors by top-N intersection\" and measure fraction in same cluster\n",
    "n_probe = min(1000, len(sampled_users))\n",
    "probe_users = np.random.choice(sampled_users, size=n_probe, replace=False)\n",
    "\n",
    "same_cluster_rates = []\n",
    "for uid in probe_users:\n",
    "    u_set = user_topn.get(uid, set())\n",
    "    if not u_set:\n",
    "        continue\n",
    "    # collect candidate users who share at least one movie\n",
    "    candidates = set()\n",
    "    for m in u_set:\n",
    "        candidates.update(movie_to_users.get(m, []))\n",
    "    candidates.discard(uid)\n",
    "    if not candidates:\n",
    "        continue\n",
    "    candidates = list(candidates)\n",
    "    # compute how many of these candidates are in same cluster as uid\n",
    "    uid_cluster = user_embed_df.loc[user_embed_df['userId']==uid, 'cluster'].values[0]\n",
    "    same = 0\n",
    "    for cand in candidates:\n",
    "        cand_cluster = user_embed_df.loc[user_embed_df['userId']==cand, 'cluster'].values[0]\n",
    "        if cand_cluster == uid_cluster:\n",
    "            same += 1\n",
    "    same_cluster_rates.append(same / len(candidates))\n",
    "\n",
    "print(f\"Avg fraction of users who share top-N items and are in same cluster: {np.mean(same_cluster_rates):.3f}\")\n",
    "\n",
    "# 5) Summarize results in a simple dict\n",
    "results_summary = {\n",
    "    \"centroid_mean_dist\": float(centroid_mean),\n",
    "    \"centroid_min_dist\": float(centroid_min),\n",
    "    \"centroid_max_dist\": float(centroid_max),\n",
    "    \"avg_intra_sim\": float(np.mean(intra_sims)),\n",
    "    \"avg_inter_sim\": float(np.mean(inter_sims)),\n",
    "    \"silhouette_sampled\": float(sil),\n",
    "    \"avg_same_cluster_rate_for_shared_topN\": float(np.mean(same_cluster_rates))\n",
    "}\n",
    "\n",
    "#print(\"Summary:\", results_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
